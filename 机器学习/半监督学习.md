# <font color ='red'>**半监督学习** *semi-supervised learning*</font>
- <font color='green'>**数据**</font>：
    - **有标记样本**：$D_l~=~\{(x_1,y_1),(x_2,y_2),...,(x_l,y_l)\}$
    - **未标记样本**：$D_u~=~\{(x_{1+l},y_{1+l}),(x_{2+l},y_{2+l}),...,(x_{l+u},y_{l+u})\}$
- <font color='green'>**优势特点**</font>：
   - 学习器 **不依赖** 外界交互
   - 自动の 利用 **未标记样本** $\underset{\Longrightarrow}{提升}$  **学习性能**

- <font color='green'>**别名**</font>：<font color = 'lightgreen'>**主动学习** </font>
  
- <font color='green'>**目标**</font>：使用尽量少の**查询**来获得尽量好の性能

- <font color='green'>**假设**</font>：
将 未标记样本 所揭示の**数据分布信息** 与 类别标记 **相联系** の假设
### <font color = 'lightgreen'>**聚类假设** </font>：
**定义**：假设数据存在 **簇结构**
### <font color = 'lightgreen'>**流行假设** </font>：
**定义**：假设数据分布在一个流行结构上 ，邻近の样本拥有相似の输出值

- <font color='green'>**种类**</font>：
### <font color = 'lightgreen'>**纯半监督学习** </font>pure semi-supervised learning：
**定义**：假设训练数据中の " **未标记样本** $\neq$ **待预测数据** "$\longrightarrow$ 开放世界
### <font color = 'lightgreen'>**直推学习** </font> transductive learning：
**定义**：假设训练数据中の " **未标记样本** $=$ **待预测数据** ""$\longrightarrow$ 封闭世界

# <font color ='red'>**生成式学习**</font>*generative methods*
- <font color='green'>**原理**</font>：**生成式模型**
- <font color='green'>**假设**</font>：
   - 所有数据（$D_l+D_u$）都是有同一个 **潜在**の **模型**生成の
   - $\Updownarrow$
   - 潜在模型の参数$\longrightarrow$联系 "**未标记数据**"+"**学习目标**"
- <font color='green'>**模型求解**</font>：
   - Step 1： 假设 **生成式模型**$$\begin{align*}&高斯混合模型：\\&\qquad\qquad\qquad p(x)~=~\underset{i=1}{\stackrel{N}{\sum}}\alpha_i·p(x|\mu_i,\Gamma_i)\end{align*}$$
   - Step 2:**EM算法** 求解参数
# <font color ='red'>**半监督SVM**</font>
<font color ='red'><font size =4>**S3VM**</font></font>

- <font color='green'>**超平面**</font>：
   - 将 $D_l$ 化分
   - 穿过 **数据低密度**区域

<font color ='red'><font size =4>**TSVM**</font></font>

- <font color='green'>**算法思想**</font>：
   - 对 $D_u$ 进行 **各种可能**の **标记指派** （<font color = 'lightgreen'>**标记指派** </font>：给数据样本贴标签/指定类别）
   - 在所有数据上寻找一个 **间隔最大化**の超平面（普通的SVM）

- <font color='green'>**目标函数**</font>：
$$
\underset{w,b,\hat{y},\xi}{min}~\frac{1}{2}||w||_2^2~+~C_l\underset{i=1}{\stackrel{l}{\sum}}\xi_i~+~\underset{i=l+1}{\stackrel{m}{\sum}}\xi_i\\
\begin{align*}
s.t.~~&y_i(w^Tx_i+b)\geq~1-\xi_i~,i=1,2,..,l~~~(D_l项约束)\\
&\hat{y}(w^Tx_i+b)\geq~1-\xi_i~,i=l+1,l+2,..,m~~~(D_u约束)\\
&\xi_i\geq 0~,~i=1,2,...,m
\end{align*}
$$
- <font color='green'>**求解过程**</font>：
   - Step 1：利用 $D_l$ 训练一个 $SVM_0$
   - Step 2：利用 $SVM_0$对$D_u$进行 **标记指派**
   - Step 3: 利用 $D_l+D_u$ 求解 **划分超平面**+**松弛变量$\xi_i$**
   - Step 4:迭代求解参数$$\begin{align*}&<1>.找出两个 标记指派为异类+很可能发生错误の未标记样本\\&<2>.交换它们的标记\\&<3>.重新求解 更新后の 化分超平面+松弛向量\\&<4>.增大 C_u的值 （初始时 C_u~<~C_l）\end{align*}$$
   - Step 5:$C_l=C_u~\Longrightarrow$  结束
- <font color='green'>解决**类别不平衡**问题</font>：
   - **拆分**：$C_u~=~C_u^+~+~C_u^-$
   - **初始化**：$$C_u^+~=~\frac{u_-}{u_+}~C_u^-$$
# <font color ='red'>**图半监督学习**</font>
- <font color='green'>**原理**</font>：将 $数据集~\Longrightarrow~图G<V,E>$
   - **V-结点集**：每个数据样本 $x_i\longrightarrow\left\{\begin{aligned}&标记样本C_l~:~染色\\&未染色样本C_u~:~未染色\\ \end{aligned}\right.$
   - **E-边集**：两个样本之间的**相似度**（强度）
- <font color='green'>**算法求解**</font>：<font color = 'lightgreen'><font size =4>**标记传播方法** </font></font>
   - <font color = 'lightgreen'><font size =4>**亲和矩阵** *$W$* </font></font>:边集Eの矩阵表示$$(W)_{ij}~=~\left\{\begin{aligned}&exp(-\frac{||x_i-x_j||_2^2}{2\sigma^2})~&,&\qquad if~i\neq j\\&\qquad\qquad 0~&,&\qquad otherwise\end{aligned}\right.=\left[\begin{matrix}&W_{ll}~,~&W_{lu}~\\&W_{ul}~,~&W_{uu}~\end{matrix}\right]$$
   - <font color = 'lightgreen'><font size =4>关于fの**能量函数**  *$E(f)$*</font></font>：$$\begin{align*}&其中：\\&\qquad\qquad f:~V\mapsto\mathbb{R} ~~:~从图G<V,E>中学到の实值函数\\&\qquad\qquad 分类规则： y_i~=~sign(f(x_i))\\&\\\end{align*}\\\begin{align*}E(f) ~&=~\frac{1}{2}~\underset{i=1}{\stackrel{m}{\sum}}~\underset{j=1}{\stackrel{m}{\sum}}~(W)_{ij}~(f(x_i)-f(x_j))^2\\&=~\frac{1}{2}(~\underset{i=1}{\stackrel{m}{\sum}}~d_if^2(x_i)~+~\underset{j=1}{\stackrel{m}{\sum}}~d_jf^2(x_j)~-~2\underset{i=1}{\stackrel{m}{\sum}}~\underset{j=1}{\stackrel{m}{\sum}}~(W)_{ij}f(x_i)f(x_j))\\&=~...\\&=~f^T(D-W)f\\&\\&其中：\\&\qquad\qquad f~=~(f_l^T,f_u^T)\\&\qquad\qquad f_l~=~(f(x_1),f(x_2),...,f(x_l))\\&\qquad\qquad f_u~=~(f(x_{l+1}),f(x_{l+2}),...,f(x_{l+u}))\\&\qquad\qquad d_i~=~\sum_{j=1}^{l+u}(W)_{ij}~~~~~第i行元素之和\\&\qquad\qquad D~=~diag(d_1,d_2,...,d_{l+u})\end{align*}$$
   - <font color = 'lightgreen'><font size =4>**拉普拉斯矩阵** *$\Delta$*</font></font>:$$\Delta~=~D~-~W$$
# <font color ='red'>**半监督聚类**</font>
- <font color='green'>**监督信息の类型**</font>：
   - <font color = 'lightgreen'><font size =4>**必连信息 $\mathcal{M}$** </font>*must-link*</font>：样本 **必须属于** **同**一个**簇**$$(x_i,x_j)~\in~\mathcal{M}~\Longrightarrow x_i与x_j~必属于~同簇$$
   - <font color = 'lightgreen'><font size =4>**勿连信息 $\mathcal{C}$** </font>*cannot-link*</font>：样本 **必不属于** 同一个 **簇** $$(x_i,x_j)~\in~\mathcal{C}~\Longrightarrow x_i与x_j~必不属于~同簇$$ $\Longrightarrow$ 监督信息为**少量**の**有标记**(簇标记)样本($D_l < D_u$)
- <font color='green'>**算法类型**</font>：

<font color = 'lightgreen'><font size =4>**约束k均值**算法 </font>*Constrained k-means*</font>

- <font color='green'>**算法流程**</font>：
$$
\begin{align*}
&假设：\\
&\qquad\qquad D~:~训练数据集\\
&\qquad\qquad \mathcal{M}~:~必连约束集合\\
&\qquad\qquad \mathcal{C}~:~勿连约束集合\\
&\qquad\qquad k~:~聚类簇数
\end{align*}
$$
   - Step 1：**初始化** ： $$\begin{align*}&D中随机选取k个样本 作为初始均值向量:\\&\qquad\qquad\qquad\{\mu_1,\mu_2,...,\mu_k\}\end{align*}$$
   - Step 2：**计算距离**:$$\begin{align*}&每个样本x_i到各个均值向量\mu_j(1\leq j\leq k)の距离：\\&\\&\qquad\qquad\qquad d_{ij}~=~||x_i-\mu_j||_2\end{align*}$$
   - Step 3：**判断**是否满足$\mathcal{C}与\mathcal{M}$**约束**：$$\begin{align*}\qquad &判断x_i加入C_k是否会破坏\mathcal{M}与\mathcal{C}约束~\\&\\&是否加入~=~\left\{\begin{aligned}&加入簇C_k~,&不违背约束\\&不加入\And返回错误提示~,&违背约束\end{aligned}\right.\end{align*}$$
   - Step 4：**更新**均值向量$$\mu_j~=~\frac{1}{|C_j|}\sum_{x\in C_j}x$$
   - Step 5:**停止迭代**$\Longrightarrow$**输出结果**$$终止条件~=~\left\{\begin{align*}&~均值向量\mu_j不再更新\\&~达到最大迭代次数T\end{align*}\right.$$

<font color = 'lightgreen'><font size =4>**约束种子k均值**算法 </font>*Constrained Seed k-means*</font>

- <font color='green'>**算法流程**</font>：
$$\begin{align*}
&假设：\\
&\qquad\qquad D~:~训练数据集\\
&\qquad\qquad S~:~少量有标记的样本\Longrightarrow~S~=~\cup_{j=1}^k~S_j\\
&\qquad\qquad k~:~聚类簇数
\end{align*}$$  
   - Step 1：**初始化** ： $$\mu_j~=~\frac{1}{|S_j|}\sum_{x\in S_j}x$$
   - Step 2：将**已标记样本**$C_l$ 归簇：$$x~\in S_j \Longrightarrow C_j~=~C_j\cup x$$
   - Step 3：将 **未标记样本** $C_u$ 归入簇：$$\begin{align*}&<1>.计算距离：~~ d_{ij}~=~||x_i-\mu_j||_2\\&<2>.找到x距离最近の簇： ~~ r=argmin_{j\in\{1,2,..,k\}}d_ij\\&<3>.将x_i归入簇： ~~C_r~=~C_r\cup x_i\end{align*}$$
   - Step 4：**更新**均值向量$$\mu_j~=~\frac{1}{|C_j|}\sum_{x\in C_j}x$$
   - Step 5:**停止迭代**$\Longrightarrow$**输出结果**$$终止条件~=~\left\{\begin{align*}&~均值向量\mu_j不再更新\\&~达到最大迭代次数T\end{align*}\right.$$
# <font color ='red'>**基于分歧の方法**</font> 